---
title: ğŸ‘€Code-4-ML-Learning
password: ""
tags:
  - äººå·¥æ™ºèƒ½
  - æœºå™¨å­¦ä¹ 
  - ä¿¡æ¯è®º
katex: false
comments: true
aside: true
date: 2022-04-11 15:07:22
cover: https://www.helloimg.com/images/2022/04/07/RsEzqc.png
top_img:
---

# Code-4-ML-Learning

<!--
 * @?: *********************************************************************
 * @Author: Weidows
 * @LastEditors: Weidows
 * @LastEditTime: 2022-02-23 02:28:46
 * @FilePath: \Blog-private\scaffolds\post.md
 * @Description:
 * @!: *********************************************************************
-->

## æ•°æ®é¢„å¤„ç†æ–¹æ³•

### æ ‡å‡†åŒ–-å‡å€¼ç§»é™¤



```python
# æ•°æ®é¢„å¤„ç†ä¹‹ï¼šå‡å€¼ç§»é™¤ç¤ºä¾‹
import numpy as np
import sklearn.preprocessing as sp

# æ ·æœ¬æ•°æ®
raw_samples = np.array([
    [3.0, -1.0, 2.0],\
    [0.0, 4.0, 3.0], \
    [1.0, -4.0, 2.0]]\
)

# æ±‚æ¯åˆ—çš„å¹³å‡å€¼ axis=0ä¸ºåˆ—, =1ä¸ºè¡Œ ä¸å¡«å°±è®¡ç®—æ‰€æœ‰å€¼
print(raw_samples.mean(axis=0))
# æ±‚æ¯åˆ—æ ‡å‡†å·®
print(raw_samples.std(axis=0))

std_samples = raw_samples.copy()  # å¤åˆ¶æ ·æœ¬æ•°æ®
for col in std_samples.T:  # .Tä¸ºè½¬ç½®,éå†æ¯åˆ—
    col_mean = col.mean()  # è®¡ç®—å¹³å‡æ•°
    col_std = col.std()  # æ±‚æ ‡å‡†å·®
    col -= col_mean  # å‡å¹³å‡å€¼
    col /= col_std  # é™¤æ ‡å‡†å·®

# å‡å€¼æ— é™è¶‹è¿‘0,ä½†å¯èƒ½ä¸æ˜¯0
print(std_samples.mean(axis=0))
print(std_samples.std(axis=0))

# scale æ ‡å‡†ç§»é™¤,ä¸ä¸Šé¢åŠŸèƒ½ç›¸åŒ
std_samples = sp.scale(raw_samples)
print(std_samples.mean(axis=0))
print(std_samples.std(axis=0))
```

    [ 1.33333333 -0.33333333  2.33333333]
    [1.24721913 3.29983165 0.47140452]
    [ 5.55111512e-17  0.00000000e+00 -2.96059473e-16]
    [1. 1. 1.]
    [ 5.55111512e-17  0.00000000e+00 -2.96059473e-16]
    [1. 1. 1.]
    

### èŒƒå›´ç¼©æ”¾



```python
# æ•°æ®é¢„å¤„ç†ä¹‹ï¼šèŒƒå›´ç¼©æ”¾
import numpy as np
import sklearn.preprocessing as sp

# æ ·æœ¬æ•°æ®
raw_samples = np.array([
    [1.0, 2.0, 3.0],\
    [4.0, 5.0, 6.0],\
    [7.0, 8.0, 9.0]]).astype("float64")

mms_samples = raw_samples.copy()  # å¤åˆ¶æ ·æœ¬æ•°æ®

for col in mms_samples.T:
    col_min = col.min()
    col_max = col.max()
    col -= col_min
    col /= (col_max - col_min)
print(mms_samples)

# æ ¹æ®ç»™å®šèŒƒå›´åˆ›å»ºä¸€ä¸ªèŒƒå›´ç¼©æ”¾å™¨å¯¹è±¡
# ä½¿ç”¨èŒƒå›´ç¼©æ”¾å™¨å®ç°ç‰¹å¾å€¼èŒƒå›´ç¼©æ”¾
mms_samples = sp.MinMaxScaler(feature_range=(0, 1))\
  .fit_transform(raw_samples)  # ç¼©æ”¾
print(mms_samples)
```

    [[0.  0.  0. ]
     [0.5 0.5 0.5]
     [1.  1.  1. ]]
    [[0.  0.  0. ]
     [0.5 0.5 0.5]
     [1.  1.  1. ]]
    

### å½’ä¸€åŒ–



```python
import numpy as np
import sklearn.preprocessing as sp

# æ ·æœ¬æ•°æ®
raw_samples = np.array([
    [10.0, 20.0, 5.0],\
    [8.0, 10.0, 1.0]])

nor_samples = raw_samples.copy()
for row in nor_samples:
    row /= abs(row).sum()  # å…ˆå¯¹è¡Œæ±‚ç»å¯¹å€¼ï¼Œå†æ±‚å’Œï¼Œå†é™¤ä»¥ç»å¯¹å€¼ä¹‹å’Œ
print(nor_samples)  # æ‰“å°ç»“æœ

# norm=l1  /=ç»å¯¹å€¼ä¹‹å’Œ
# norm=l2  /=å¹³æ–¹ä¹‹å’Œ
nor_samples = sp.normalize(raw_samples.copy(), norm='l1')
print(nor_samples)  # æ‰“å°ç»“æœ

```

    [[0.28571429 0.57142857 0.14285714]
     [0.42105263 0.52631579 0.05263158]]
    [[0.28571429 0.57142857 0.14285714]
     [0.42105263 0.52631579 0.05263158]]
    

### äºŒå€¼åŒ–


```python
# äºŒå€¼åŒ–
import numpy as np
import sklearn.preprocessing as sp

raw_samples = np.array([[65.5, 89.0, 73.0],\
                        [55.0, 99.0, 98.5],\
                        [45.0, 22.5, 60.0]])

bin_samples = raw_samples.copy()  # å¤åˆ¶æ•°ç»„
# ç”Ÿæˆæ©ç æ•°ç»„
mask1 = bin_samples < 60
print(mask1)
mask2 = bin_samples >= 60

# é€šè¿‡æ©ç è¿›è¡ŒäºŒå€¼åŒ–å¤„ç† (åªè½¬æ¢Trueçš„ä½ç½®)
bin_samples[mask1] = 0
print(bin_samples)
bin_samples[mask2] = 1

print(bin_samples)  # æ‰“å°ç»“æœ

bin_transformer = sp.Binarizer(threshold=60 - 1)  # åˆ›å»ºäºŒå€¼åŒ–å¯¹è±¡(æ³¨æ„è¾¹ç•Œå€¼)
bin_samples = bin_transformer.transform(raw_samples.copy())  # äºŒå€¼åŒ–é¢„å¤„ç†
print(bin_samples)
```

    [[False False False]
     [ True False False]
     [ True  True False]]
    [[65.5 89.  73. ]
     [ 0.  99.  98.5]
     [ 0.   0.  60. ]]
    [[1. 1. 1.]
     [0. 1. 1.]
     [0. 0. 1.]]
    [[1. 1. 1.]
     [0. 1. 1.]
     [0. 0. 1.]]
    

### ç‹¬çƒ­ç¼–ç ç¤ºä¾‹



```python
import numpy as np
import sklearn.preprocessing as sp

raw_samples = np.array([[1, 3, 2],\
                        [7, 5, 4],\
                        [1, 8, 6],\
                        [7, 3, 9]])

one_hot_encoder = sp.OneHotEncoder(
    sparse=False,  # æ˜¯å¦é‡‡ç”¨ç¨€ç–æ ¼å¼
    dtype="int32",
    categories="auto")  # è‡ªåŠ¨ç¼–ç 

# æ‰§è¡Œç‹¬çƒ­ç¼–ç 
oh_samples = one_hot_encoder.fit_transform(raw_samples.copy())
print(oh_samples)

print(one_hot_encoder.inverse_transform(oh_samples))  # è§£ç 

```

    [[1 0 1 0 0 1 0 0 0]
     [0 1 0 1 0 0 1 0 0]
     [1 0 0 0 1 0 0 1 0]
     [0 1 1 0 0 0 0 0 1]]
    [[1 3 2]
     [7 5 4]
     [1 8 6]
     [7 3 9]]
    

### æ ‡ç­¾ç¼–ç 



```python
import numpy as np
import sklearn.preprocessing as sp

raw_samples = np.array(['audi', 'ford', 'audi', 'bmw', 'ford', 'bmw'])

lb_encoder = sp.LabelEncoder()  # å®šä¹‰æ ‡ç­¾ç¼–ç å¯¹è±¡
lb_samples = lb_encoder.fit_transform(raw_samples.copy())  # æ‰§è¡Œæ ‡ç­¾ç¼–ç 
print(lb_samples)

print(lb_encoder.inverse_transform(lb_samples))  # é€†å‘è½¬æ¢

```

    [0 2 0 1 2 1]
    ['audi' 'ford' 'audi' 'bmw' 'ford' 'bmw']
    

## åŸºæœ¬é—®é¢˜

### å›å½’é—®é¢˜

#### çº¿æ€§å›å½’



```python
import numpy as np
import matplotlib.pyplot as mp
from mpl_toolkits.mplot3d import axes3d
import sklearn.preprocessing as sp

# è®­ç»ƒæ•°æ®é›†
train_x = np.array([0.5, 0.6, 0.8, 1.1, 1.4])  # è¾“å…¥é›†
train_y = np.array([5.0, 5.5, 6.0, 6.8, 7.0])  # è¾“å‡ºé›†

n_epochs = 30  # è¿­ä»£æ¬¡æ•°
l_rate = 0.01  # å­¦ä¹ ç‡
epochs = []  # è®°å½•è¿­ä»£æ¬¡æ•°
losses = []  # è®°å½•æŸå¤±å€¼

w0, w1 = [1], [1]  # æ¨¡å‹åˆå§‹å€¼

for i in range(1, n_epochs + 1):
    epochs.append(i)  # è®°å½•ç¬¬å‡ æ¬¡è¿­ä»£

    y = w0[-1] + w1[-1] * train_x  # å–å‡ºæœ€æ–°çš„w0,w1è®¡ç®—çº¿æ€§æ–¹ç¨‹è¾“å‡º
    # æŸå¤±å‡½æ•°(å‡æ–¹å·®)
    loss = (((train_y - y)**2).sum()) / 2
    losses.append(loss)  # è®°å½•æ¯æ¬¡è¿­ä»£çš„æŸå¤±å€¼

    print("%d: w0=%f, w1=%f, loss=%f" % (i, w0[-1], w1[-1], loss))

    # è®¡ç®—w0,w1çš„åå¯¼æ•°
    d0 = -(train_y - y).sum()
    d1 = -(train_x * (train_y - y)).sum()

    # æ›´æ–°w0,w1
    w0.append(w0[-1] - (d0 * l_rate))
    w1.append(w1[-1] - (d1 * l_rate))


###################### è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ– ######################
## æŸå¤±å‡½æ•°æ”¶æ•›è¿‡ç¨‹
w0 = np.array(w0[:-1])
w1 = np.array(w1[:-1])

mp.figure("Losses", facecolor="lightgray")  # åˆ›å»ºä¸€ä¸ªçª—ä½“
mp.title("epoch", fontsize=20)
mp.ylabel("loss", fontsize=14)
mp.grid(linestyle=":")  # ç½‘æ ¼çº¿ï¼šè™šçº¿
mp.plot(epochs, losses, c="blue", label="loss")
mp.legend()  # å›¾ä¾‹
mp.tight_layout()  # ç´§å‡‘æ ¼å¼

## æ˜¾ç¤ºæ¨¡å‹ç›´çº¿
pred_y = w0[-1] + w1[-1] * train_x  # æ ¹æ®xé¢„æµ‹y
mp.figure("Linear Regression", facecolor="lightgray")
mp.title("Linear Regression", fontsize=20)
mp.xlabel("x", fontsize=14)
mp.ylabel("y", fontsize=14)
mp.grid(linestyle=":")
mp.scatter(train_x, train_y, c="blue", label="Traing")  # ç»˜åˆ¶æ ·æœ¬æ•£ç‚¹å›¾
mp.plot(train_x, pred_y, c="red", label="Regression")
mp.legend()

# æ˜¾ç¤ºæ¢¯åº¦ä¸‹é™è¿‡ç¨‹(å¤åˆ¶ç²˜è´´å³å¯ï¼Œä¸éœ€è¦ç¼–å†™)
# è®¡ç®—æŸå¤±å‡½æ•°æ›²é¢ä¸Šçš„ç‚¹ loss = f(w0, w1)
arr1 = np.linspace(0, 10, 500)  # 0~9é—´äº§ç”Ÿ500ä¸ªå…ƒç´ çš„å‡åŒ€åˆ—è¡¨
arr2 = np.linspace(0, 3.5, 500)  # 0~3.5é—´äº§ç”Ÿ500ä¸ªå…ƒç´ çš„å‡åŒ€åˆ—è¡¨

grid_w0, grid_w1 = np.meshgrid(arr1, arr2)  # äº§ç”ŸäºŒç»´çŸ©é˜µ

flat_w0, flat_w1 = grid_w0.ravel(), grid_w1.ravel()  # äºŒç»´çŸ©é˜µæ‰å¹³åŒ–
loss_metrix = train_y.reshape(-1, 1)  # ç”Ÿæˆè¯¯å·®çŸ©é˜µï¼ˆ-1,1ï¼‰è¡¨ç¤ºè‡ªåŠ¨è®¡ç®—ç»´åº¦
outer = np.outer(train_x, flat_w1)  # æ±‚å¤–ç§¯ï¼ˆtrain_xå’Œflat_w1å…ƒç´ ä¸¤ä¸¤ç›¸ä¹˜çš„æ–°çŸ©é˜µï¼‰
# è®¡ç®—æŸå¤±ï¼š((w0 + w1*x - y)**2)/2
flat_loss = (((flat_w0 + outer - loss_metrix) ** 2).sum(axis=0)) / 2
grid_loss = flat_loss.reshape(grid_w0.shape)

mp.figure('Loss Function')
ax = mp.axes(projection='3d')
mp.title('Loss Function', fontsize=14)
ax.set_xlabel('w0', fontsize=14)
ax.set_ylabel('w1', fontsize=14)
ax.set_zlabel('loss', fontsize=14)
ax.plot_surface(grid_w0, grid_w1, grid_loss, rstride=10, cstride=10, cmap='jet')
ax.plot(w0, w1, losses, 'o-', c='orangered', label='BGD', zorder=5)
mp.legend(loc='lower left')

mp.show()
```

    1: w0=1.000000, w1=1.000000, loss=44.175000
    2: w0=1.209000, w1=1.190600, loss=36.538828
    3: w0=1.399164, w1=1.363579, loss=30.231687
    4: w0=1.572208, w1=1.520546, loss=25.022227
    5: w0=1.729693, w1=1.662961, loss=20.719373
    6: w0=1.873039, w1=1.792151, loss=17.165309
    7: w0=2.003532, w1=1.909325, loss=14.229691
    8: w0=2.122345, w1=2.015577, loss=11.804865
    9: w0=2.230542, w1=2.111905, loss=9.801916
    10: w0=2.329091, w1=2.199215, loss=8.147408
    11: w0=2.418871, w1=2.278330, loss=6.780688
    12: w0=2.500681, w1=2.349997, loss=5.651660
    13: w0=2.575247, w1=2.414898, loss=4.718950
    14: w0=2.643230, w1=2.473648, loss=3.948384
    15: w0=2.705228, w1=2.526811, loss=3.311740
    16: w0=2.761786, w1=2.574896, loss=2.785706
    17: w0=2.813402, w1=2.618367, loss=2.351029
    18: w0=2.860524, w1=2.657645, loss=1.991807
    19: w0=2.903561, w1=2.693114, loss=1.694907
    20: w0=2.942886, w1=2.725122, loss=1.449482
    21: w0=2.978836, w1=2.753985, loss=1.246572
    22: w0=3.011719, w1=2.779990, loss=1.078777
    23: w0=3.041814, w1=2.803399, loss=0.939987
    24: w0=3.069373, w1=2.824449, loss=0.825153
    25: w0=3.094629, w1=2.843355, loss=0.730107
    26: w0=3.117790, w1=2.860315, loss=0.651405
    27: w0=3.139046, w1=2.875507, loss=0.586204
    28: w0=3.158572, w1=2.889091, loss=0.532154
    29: w0=3.176523, w1=2.901216, loss=0.487315
    30: w0=3.193044, w1=2.912016, loss=0.450086
    


    
![png](ML_files/ML_13_1.png)
    



    
![png](ML_files/ML_13_2.png)
    



    
![png](ML_files/ML_13_3.png)
    



```python
# åˆ©ç”¨LinearRegressionå®ç°çº¿æ€§å›å½’
import numpy as np
import sklearn.linear_model as lm  # çº¿æ€§æ¨¡å‹# çº¿æ€§æ¨¡å‹
import sklearn.metrics as sm  # æ¨¡å‹æ€§èƒ½è¯„ä»·æ¨¡å—
import matplotlib.pyplot as mp

train_x = np.array([[0.5], [0.6], [0.8], [1.1], [1.4]])  # è¾“å…¥é›†
train_y = np.array([5.0, 5.5, 6.0, 6.8, 7.0])  # è¾“å‡ºé›†

# åˆ›å»ºçº¿æ€§å›å½’å™¨
line_model = lm.LinearRegression()
# ç”¨å·²çŸ¥è¾“å…¥ã€è¾“å‡ºæ•°æ®é›†è®­ç»ƒå›å½’å™¨
line_model.fit(train_x, train_y)
# æ ¹æ®è®­ç»ƒæ¨¡å‹é¢„æµ‹è¾“å‡º
pred_y = line_model.predict(train_x)

print("coef_:", line_model.coef_)  # ç³»æ•°
print("intercept_:", line_model.intercept_)  # æˆªè·

# å¯è§†åŒ–å›å½’æ›²çº¿
mp.figure('Linear Regression', facecolor='lightgray')
mp.title('Linear Regression', fontsize=20)
mp.xlabel('x', fontsize=14)
mp.ylabel('y', fontsize=14)
mp.tick_params(labelsize=10)
mp.grid(linestyle=':')

# ç»˜åˆ¶æ ·æœ¬ç‚¹
mp.scatter(train_x, train_y, c='blue', alpha=0.8, s=60, label='Sample')

# ç»˜åˆ¶æ‹Ÿåˆç›´çº¿
mp.plot(
    train_x,  # xåæ ‡æ•°æ®
    pred_y,  # yåæ ‡æ•°æ®
    c='orangered',
    label='Regression Line')

mp.legend()  # å·¦ä¸Šè§’çš„å›¾ä¾‹
mp.show()
```

    coef_: [2.2189781]
    intercept_: 4.107299270072994
    


    
![png](ML_files/ML_14_1.png)
    


## ä»£ç ç›¸å…³

### å­˜å‚¨-è¯»å–æ¨¡å‹



```python
import numpy as np
import sklearn.linear_model as lm  # çº¿æ€§æ¨¡å‹
import sklearn.metrics as sm  # æ¨¡å‹æ€§èƒ½è¯„ä»·æ¨¡å—
import matplotlib.pyplot as mp
import pickle

x = np.array([[0.5], [0.6], [0.8], [1.1], [1.4]])  # è¾“å…¥é›†
y = np.array([5.0, 5.5, 6.0, 6.8, 7.0])  # è¾“å‡ºé›†

# åˆ›å»ºçº¿æ€§å›å½’å™¨
model = lm.LinearRegression()
# ç”¨å·²çŸ¥è¾“å…¥ã€è¾“å‡ºæ•°æ®é›†è®­ç»ƒå›å½’å™¨
model.fit(x, y)

print("è®­ç»ƒå®Œæˆ.")

# ä¿å­˜è®­ç»ƒåçš„æ¨¡å‹
with open('linear_model.pkl', 'wb') as f:
    pickle.dump(model, f)
    print("ä¿å­˜æ¨¡å‹å®Œæˆ.")


######################### åŠ è½½æ¨¡å‹ #########################
# ä¸Šé¢é€šè¿‡è®­ç»ƒæ•°æ®x,y è®­ç»ƒå¥½äº† x -> y çš„çº¿æ€§å›å½’æ¨¡å‹
# ä¸‹é¢åŠ è½½æ¨¡å‹, å†ç»™å‡ºæµ‹è¯•æ•°æ® x, æŸ¥çœ‹æ¨¡å‹é¢„æµ‹ç»“æœç›´çº¿ä¸åŸå§‹æ•°æ® (x,y) çš„æ‹Ÿåˆåº¦
with open('linear_model.pkl', 'rb') as f:
    model = pickle.load(f)
    print("åŠ è½½æ¨¡å‹å®Œæˆ.")

# æ ¹æ®åŠ è½½çš„æ¨¡å‹é¢„æµ‹è¾“å‡º
pred_y = model.predict(x)

# å¯è§†åŒ–å›å½’æ›²çº¿
mp.figure('Linear Regression', facecolor='lightgray')
mp.title('Linear Regression', fontsize=20)
mp.xlabel('x', fontsize=14)
mp.ylabel('y', fontsize=14)
mp.tick_params(labelsize=10)
mp.grid(linestyle=':')
mp.scatter(x, y, c='blue', alpha=0.8, s=60, label='Sample Points')

mp.plot(x, pred_y, c='orangered', label='Regression')

mp.legend()
mp.show()
```

    è®­ç»ƒå®Œæˆ.
    ä¿å­˜æ¨¡å‹å®Œæˆ.
    åŠ è½½æ¨¡å‹å®Œæˆ.
    


    
![png](ML_files/ML_16_1.png)
    


## ä¿¡æ¯è®º

### ä¸¤ç‚¹åˆ†å¸ƒä¿¡æ¯ç†µ



```python
import numpy as np
import matplotlib.pyplot as plt

# å› ä¸º ln0 æ— å®šä¹‰, ç”¨æ­¤å€¼æ¨¡æ‹Ÿè¶‹è¿‘ 0
eps = 1e-5

# probability
p = np.linspace(eps, 1 - eps, 100)

# Information entropy
h = -(1 - p) * np.log2(1 - p) - p * np.log2(p)

plt.plot(p, h, label='Information entropy', color='red', lw=3)
plt.xlabel('Probability', fontsize=16)
plt.ylabel('Entropy', fontsize=16)
plt.legend(loc='best', fontsize=16)
plt.grid(True)
plt.show()

# ç»“æœä¸­ä¿¡æ¯ç†µçš„å³°å€¼å–å†³äº log åº•æ•°, eä¸ºåº•æ—¶å³°å€¼ä¸º0.7å·¦å³, 2ä¸ºåº•å³°å€¼ä¸º1
```


    
![png](ML_files/ML_18_0.png)
    


<a>![åˆ†å‰²çº¿](https://cdn.jsdelivr.net/gh/Weidows/Images/img/divider.png)</a>

## å€Ÿç‰©è¡¨

<a name='cite_note-1' href='#cite_ref-1'>[1]</a>: https://discover304.top/

