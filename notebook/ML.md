---
title: ğŸ‘€Code-4-ML-Learning
password: ""
tags:
  - äººå·¥æ™ºèƒ½
  - æœºå™¨å­¦ä¹ 
  - ä¿¡æ¯è®º
katex: false
comments: true
aside: true
date: 2022-04-11 15:07:22
cover: https://www.helloimg.com/images/2022/04/07/RsEzqc.png
top_img:
---

# Code-4-ML-Learning

<!--
 * @?: *********************************************************************
 * @Author: Weidows
 * @LastEditors: Weidows
 * @LastEditTime: 2022-02-23 02:28:46
 * @FilePath: \Blog-private\scaffolds\post.md
 * @Description:
 * @!: *********************************************************************
-->

## åº

æ­¤æ–‡ä¸ºå…¶ä»–æ–‡ç« çš„ä»£ç éƒ¨åˆ†:

> [ğŸŠAll-about-AI](../../../AI/AI)

ä¹Ÿæä¾›äº† notebook å½¢å¼: [ä»£ç åœ°å€](https://github.com/Weidows-projects/public-post/blob/main/notebook/ML/ML.ipynb)

<a>![åˆ†å‰²çº¿](https://cdn.jsdelivr.net/gh/Weidows/Images/img/divider.png)</a>

## æ•°æ®é¢„å¤„ç†æ–¹æ³•

### æ ‡å‡†åŒ–-å‡å€¼ç§»é™¤



```python
# æ•°æ®é¢„å¤„ç†ä¹‹ï¼šå‡å€¼ç§»é™¤ç¤ºä¾‹
import numpy as np
import sklearn.preprocessing as sp

# æ ·æœ¬æ•°æ®
raw_samples = np.array([
    [3.0, -1.0, 2.0],\
    [0.0, 4.0, 3.0], \
    [1.0, -4.0, 2.0]]\
)

# æ±‚æ¯åˆ—çš„å¹³å‡å€¼ axis=0ä¸ºåˆ—, =1ä¸ºè¡Œ ä¸å¡«å°±è®¡ç®—æ‰€æœ‰å€¼
print(raw_samples.mean(axis=0))
# æ±‚æ¯åˆ—æ ‡å‡†å·®
print(raw_samples.std(axis=0))

std_samples = raw_samples.copy()  # å¤åˆ¶æ ·æœ¬æ•°æ®
for col in std_samples.T:  # .Tä¸ºè½¬ç½®,éå†æ¯åˆ—
    col_mean = col.mean()  # è®¡ç®—å¹³å‡æ•°
    col_std = col.std()  # æ±‚æ ‡å‡†å·®
    col -= col_mean  # å‡å¹³å‡å€¼
    col /= col_std  # é™¤æ ‡å‡†å·®

# å‡å€¼æ— é™è¶‹è¿‘0,ä½†å¯èƒ½ä¸æ˜¯0
print(std_samples.mean(axis=0))
print(std_samples.std(axis=0))

# scale æ ‡å‡†ç§»é™¤,ä¸ä¸Šé¢åŠŸèƒ½ç›¸åŒ
std_samples = sp.scale(raw_samples)
print(std_samples.mean(axis=0))
print(std_samples.std(axis=0))
```

    [ 1.33333333 -0.33333333  2.33333333]
    [1.24721913 3.29983165 0.47140452]
    [ 5.55111512e-17  0.00000000e+00 -2.96059473e-16]
    [1. 1. 1.]
    [ 5.55111512e-17  0.00000000e+00 -2.96059473e-16]
    [1. 1. 1.]
    

### èŒƒå›´ç¼©æ”¾



```python
# æ•°æ®é¢„å¤„ç†ä¹‹ï¼šèŒƒå›´ç¼©æ”¾
import numpy as np
import sklearn.preprocessing as sp

# æ ·æœ¬æ•°æ®
raw_samples = np.array([
    [1.0, 2.0, 3.0],\
    [4.0, 5.0, 6.0],\
    [7.0, 8.0, 9.0]]).astype("float64")

mms_samples = raw_samples.copy()  # å¤åˆ¶æ ·æœ¬æ•°æ®

for col in mms_samples.T:
    col_min = col.min()
    col_max = col.max()
    col -= col_min
    col /= (col_max - col_min)
print(mms_samples)

# æ ¹æ®ç»™å®šèŒƒå›´åˆ›å»ºä¸€ä¸ªèŒƒå›´ç¼©æ”¾å™¨å¯¹è±¡
# ä½¿ç”¨èŒƒå›´ç¼©æ”¾å™¨å®ç°ç‰¹å¾å€¼èŒƒå›´ç¼©æ”¾
mms_samples = sp.MinMaxScaler(feature_range=(0, 1))\
  .fit_transform(raw_samples)  # ç¼©æ”¾
print(mms_samples)
```

    [[0.  0.  0. ]
     [0.5 0.5 0.5]
     [1.  1.  1. ]]
    [[0.  0.  0. ]
     [0.5 0.5 0.5]
     [1.  1.  1. ]]
    

### å½’ä¸€åŒ–



```python
import numpy as np
import sklearn.preprocessing as sp

# æ ·æœ¬æ•°æ®
raw_samples = np.array([
    [10.0, 20.0, 5.0],\
    [8.0, 10.0, 1.0]])

nor_samples = raw_samples.copy()
for row in nor_samples:
    row /= abs(row).sum()  # å…ˆå¯¹è¡Œæ±‚ç»å¯¹å€¼ï¼Œå†æ±‚å’Œï¼Œå†é™¤ä»¥ç»å¯¹å€¼ä¹‹å’Œ
print(nor_samples)  # æ‰“å°ç»“æœ

# norm=l1  /=ç»å¯¹å€¼ä¹‹å’Œ
# norm=l2  /=å¹³æ–¹ä¹‹å’Œ
nor_samples = sp.normalize(raw_samples.copy(), norm='l1')
print(nor_samples)  # æ‰“å°ç»“æœ

```

    [[0.28571429 0.57142857 0.14285714]
     [0.42105263 0.52631579 0.05263158]]
    [[0.28571429 0.57142857 0.14285714]
     [0.42105263 0.52631579 0.05263158]]
    

### äºŒå€¼åŒ–


```python
# äºŒå€¼åŒ–
import numpy as np
import sklearn.preprocessing as sp

raw_samples = np.array([[65.5, 89.0, 73.0],\
                        [55.0, 99.0, 98.5],\
                        [45.0, 22.5, 60.0]])

bin_samples = raw_samples.copy()  # å¤åˆ¶æ•°ç»„
# ç”Ÿæˆæ©ç æ•°ç»„
mask1 = bin_samples < 60
print(mask1)
mask2 = bin_samples >= 60

# é€šè¿‡æ©ç è¿›è¡ŒäºŒå€¼åŒ–å¤„ç† (åªè½¬æ¢Trueçš„ä½ç½®)
bin_samples[mask1] = 0
print(bin_samples)
bin_samples[mask2] = 1

print(bin_samples)  # æ‰“å°ç»“æœ

bin_transformer = sp.Binarizer(threshold=60 - 1)  # åˆ›å»ºäºŒå€¼åŒ–å¯¹è±¡(æ³¨æ„è¾¹ç•Œå€¼)
bin_samples = bin_transformer.transform(raw_samples.copy())  # äºŒå€¼åŒ–é¢„å¤„ç†
print(bin_samples)
```

    [[False False False]
     [ True False False]
     [ True  True False]]
    [[65.5 89.  73. ]
     [ 0.  99.  98.5]
     [ 0.   0.  60. ]]
    [[1. 1. 1.]
     [0. 1. 1.]
     [0. 0. 1.]]
    [[1. 1. 1.]
     [0. 1. 1.]
     [0. 0. 1.]]
    

### ç‹¬çƒ­ç¼–ç ç¤ºä¾‹



```python
import numpy as np
import sklearn.preprocessing as sp

raw_samples = np.array([[1, 3, 2],\
                        [7, 5, 4],\
                        [1, 8, 6],\
                        [7, 3, 9]])

one_hot_encoder = sp.OneHotEncoder(
    sparse=False,  # æ˜¯å¦é‡‡ç”¨ç¨€ç–æ ¼å¼
    dtype="int32",
    categories="auto")  # è‡ªåŠ¨ç¼–ç 

# æ‰§è¡Œç‹¬çƒ­ç¼–ç 
oh_samples = one_hot_encoder.fit_transform(raw_samples.copy())
print(oh_samples)

print(one_hot_encoder.inverse_transform(oh_samples))  # è§£ç 

```

    [[1 0 1 0 0 1 0 0 0]
     [0 1 0 1 0 0 1 0 0]
     [1 0 0 0 1 0 0 1 0]
     [0 1 1 0 0 0 0 0 1]]
    [[1 3 2]
     [7 5 4]
     [1 8 6]
     [7 3 9]]
    

### æ ‡ç­¾ç¼–ç 



```python
import numpy as np
import sklearn.preprocessing as sp

raw_samples = np.array(['audi', 'ford', 'audi', 'bmw', 'ford', 'bmw'])

lb_encoder = sp.LabelEncoder()  # å®šä¹‰æ ‡ç­¾ç¼–ç å¯¹è±¡
lb_samples = lb_encoder.fit_transform(raw_samples.copy())  # æ‰§è¡Œæ ‡ç­¾ç¼–ç 
print(lb_samples)

print(lb_encoder.inverse_transform(lb_samples))  # é€†å‘è½¬æ¢

```

    [0 2 0 1 2 1]
    ['audi' 'ford' 'audi' 'bmw' 'ford' 'bmw']
    

## åŸºæœ¬é—®é¢˜

### å›å½’é—®é¢˜

#### çº¿æ€§å›å½’



```python
import numpy as np
import matplotlib.pyplot as mp
from mpl_toolkits.mplot3d import axes3d
import sklearn.preprocessing as sp

# è®­ç»ƒæ•°æ®é›†
train_x = np.array([0.5, 0.6, 0.8, 1.1, 1.4])  # è¾“å…¥é›†
train_y = np.array([5.0, 5.5, 6.0, 6.8, 7.0])  # è¾“å‡ºé›†

n_epochs = 30  # è¿­ä»£æ¬¡æ•°
l_rate = 0.01  # å­¦ä¹ ç‡
epochs = []  # è®°å½•è¿­ä»£æ¬¡æ•°
losses = []  # è®°å½•æŸå¤±å€¼

w0, w1 = [1], [1]  # æ¨¡å‹åˆå§‹å€¼

for i in range(1, n_epochs + 1):
    epochs.append(i)  # è®°å½•ç¬¬å‡ æ¬¡è¿­ä»£

    y = w0[-1] + w1[-1] * train_x  # å–å‡ºæœ€æ–°çš„w0,w1è®¡ç®—çº¿æ€§æ–¹ç¨‹è¾“å‡º
    # æŸå¤±å‡½æ•°(å‡æ–¹å·®)
    loss = (((train_y - y)**2).sum()) / 2
    losses.append(loss)  # è®°å½•æ¯æ¬¡è¿­ä»£çš„æŸå¤±å€¼

    print("%d: w0=%f, w1=%f, loss=%f" % (i, w0[-1], w1[-1], loss))

    # è®¡ç®—w0,w1çš„åå¯¼æ•°
    d0 = -(train_y - y).sum()
    d1 = -(train_x * (train_y - y)).sum()

    # æ›´æ–°w0,w1
    w0.append(w0[-1] - (d0 * l_rate))
    w1.append(w1[-1] - (d1 * l_rate))


###################### è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ– ######################
## æŸå¤±å‡½æ•°æ”¶æ•›è¿‡ç¨‹
w0 = np.array(w0[:-1])
w1 = np.array(w1[:-1])

mp.figure("Losses", facecolor="lightgray")  # åˆ›å»ºä¸€ä¸ªçª—ä½“
mp.title("epoch", fontsize=20)
mp.ylabel("loss", fontsize=14)
mp.grid(linestyle=":")  # ç½‘æ ¼çº¿ï¼šè™šçº¿
mp.plot(epochs, losses, c="blue", label="loss")
mp.legend()  # å›¾ä¾‹
mp.tight_layout()  # ç´§å‡‘æ ¼å¼

## æ˜¾ç¤ºæ¨¡å‹ç›´çº¿
pred_y = w0[-1] + w1[-1] * train_x  # æ ¹æ®xé¢„æµ‹y
mp.figure("Linear Regression", facecolor="lightgray")
mp.title("Linear Regression", fontsize=20)
mp.xlabel("x", fontsize=14)
mp.ylabel("y", fontsize=14)
mp.grid(linestyle=":")
mp.scatter(train_x, train_y, c="blue", label="Traing")  # ç»˜åˆ¶æ ·æœ¬æ•£ç‚¹å›¾
mp.plot(train_x, pred_y, c="red", label="Regression")
mp.legend()

# æ˜¾ç¤ºæ¢¯åº¦ä¸‹é™è¿‡ç¨‹(å¤åˆ¶ç²˜è´´å³å¯ï¼Œä¸éœ€è¦ç¼–å†™)
# è®¡ç®—æŸå¤±å‡½æ•°æ›²é¢ä¸Šçš„ç‚¹ loss = f(w0, w1)
arr1 = np.linspace(0, 10, 500)  # 0~9é—´äº§ç”Ÿ500ä¸ªå…ƒç´ çš„å‡åŒ€åˆ—è¡¨
arr2 = np.linspace(0, 3.5, 500)  # 0~3.5é—´äº§ç”Ÿ500ä¸ªå…ƒç´ çš„å‡åŒ€åˆ—è¡¨

grid_w0, grid_w1 = np.meshgrid(arr1, arr2)  # äº§ç”ŸäºŒç»´çŸ©é˜µ

flat_w0, flat_w1 = grid_w0.ravel(), grid_w1.ravel()  # äºŒç»´çŸ©é˜µæ‰å¹³åŒ–
loss_metrix = train_y.reshape(-1, 1)  # ç”Ÿæˆè¯¯å·®çŸ©é˜µï¼ˆ-1,1ï¼‰è¡¨ç¤ºè‡ªåŠ¨è®¡ç®—ç»´åº¦
outer = np.outer(train_x, flat_w1)  # æ±‚å¤–ç§¯ï¼ˆtrain_xå’Œflat_w1å…ƒç´ ä¸¤ä¸¤ç›¸ä¹˜çš„æ–°çŸ©é˜µï¼‰
# è®¡ç®—æŸå¤±ï¼š((w0 + w1*x - y)**2)/2
flat_loss = (((flat_w0 + outer - loss_metrix) ** 2).sum(axis=0)) / 2
grid_loss = flat_loss.reshape(grid_w0.shape)

mp.figure('Loss Function')
ax = mp.axes(projection='3d')
mp.title('Loss Function', fontsize=14)
ax.set_xlabel('w0', fontsize=14)
ax.set_ylabel('w1', fontsize=14)
ax.set_zlabel('loss', fontsize=14)
ax.plot_surface(grid_w0, grid_w1, grid_loss, rstride=10, cstride=10, cmap='jet')
ax.plot(w0, w1, losses, 'o-', c='orangered', label='BGD', zorder=5)
mp.legend(loc='lower left')

mp.show()
```

    1: w0=1.000000, w1=1.000000, loss=44.175000
    2: w0=1.209000, w1=1.190600, loss=36.538828
    3: w0=1.399164, w1=1.363579, loss=30.231687
    4: w0=1.572208, w1=1.520546, loss=25.022227
    5: w0=1.729693, w1=1.662961, loss=20.719373
    6: w0=1.873039, w1=1.792151, loss=17.165309
    7: w0=2.003532, w1=1.909325, loss=14.229691
    8: w0=2.122345, w1=2.015577, loss=11.804865
    9: w0=2.230542, w1=2.111905, loss=9.801916
    10: w0=2.329091, w1=2.199215, loss=8.147408
    11: w0=2.418871, w1=2.278330, loss=6.780688
    12: w0=2.500681, w1=2.349997, loss=5.651660
    13: w0=2.575247, w1=2.414898, loss=4.718950
    14: w0=2.643230, w1=2.473648, loss=3.948384
    15: w0=2.705228, w1=2.526811, loss=3.311740
    16: w0=2.761786, w1=2.574896, loss=2.785706
    17: w0=2.813402, w1=2.618367, loss=2.351029
    18: w0=2.860524, w1=2.657645, loss=1.991807
    19: w0=2.903561, w1=2.693114, loss=1.694907
    20: w0=2.942886, w1=2.725122, loss=1.449482
    21: w0=2.978836, w1=2.753985, loss=1.246572
    22: w0=3.011719, w1=2.779990, loss=1.078777
    23: w0=3.041814, w1=2.803399, loss=0.939987
    24: w0=3.069373, w1=2.824449, loss=0.825153
    25: w0=3.094629, w1=2.843355, loss=0.730107
    26: w0=3.117790, w1=2.860315, loss=0.651405
    27: w0=3.139046, w1=2.875507, loss=0.586204
    28: w0=3.158572, w1=2.889091, loss=0.532154
    29: w0=3.176523, w1=2.901216, loss=0.487315
    30: w0=3.193044, w1=2.912016, loss=0.450086
    


    
![png](ML_files/ML_13_1.png)
    



    
![png](ML_files/ML_13_2.png)
    



    
![png](ML_files/ML_13_3.png)
    



```python
# åˆ©ç”¨LinearRegressionå®ç°çº¿æ€§å›å½’
import numpy as np
import sklearn.linear_model as lm  # çº¿æ€§æ¨¡å‹# çº¿æ€§æ¨¡å‹
import sklearn.metrics as sm  # æ¨¡å‹æ€§èƒ½è¯„ä»·æ¨¡å—
import matplotlib.pyplot as mp

train_x = np.array([[0.5], [0.6], [0.8], [1.1], [1.4]])  # è¾“å…¥é›†
train_y = np.array([5.0, 5.5, 6.0, 6.8, 7.0])  # è¾“å‡ºé›†

# åˆ›å»ºçº¿æ€§å›å½’å™¨
line_model = lm.LinearRegression()
# ç”¨å·²çŸ¥è¾“å…¥ã€è¾“å‡ºæ•°æ®é›†è®­ç»ƒå›å½’å™¨
line_model.fit(train_x, train_y)
# æ ¹æ®è®­ç»ƒæ¨¡å‹é¢„æµ‹è¾“å‡º
pred_y = line_model.predict(train_x)

print("coef_:", line_model.coef_)  # ç³»æ•°
print("intercept_:", line_model.intercept_)  # æˆªè·

# å¯è§†åŒ–å›å½’æ›²çº¿
mp.figure('Linear Regression', facecolor='lightgray')
mp.title('Linear Regression', fontsize=20)
mp.xlabel('x', fontsize=14)
mp.ylabel('y', fontsize=14)
mp.tick_params(labelsize=10)
mp.grid(linestyle=':')

# ç»˜åˆ¶æ ·æœ¬ç‚¹
mp.scatter(train_x, train_y, c='blue', alpha=0.8, s=60, label='Sample')

# ç»˜åˆ¶æ‹Ÿåˆç›´çº¿
mp.plot(
    train_x,  # xåæ ‡æ•°æ®
    pred_y,  # yåæ ‡æ•°æ®
    c='orangered',
    label='Regression Line')

mp.legend()  # å·¦ä¸Šè§’çš„å›¾ä¾‹
mp.show()
```

    coef_: [2.2189781]
    intercept_: 4.107299270072994
    


    
![png](ML_files/ML_14_1.png)
    


### åˆ†ç±»é—®é¢˜

#### å†³ç­–æ ‘åˆ†ç±»



```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib as mpl
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder
import pydotplus

mpl.rcParams['font.sans-serif'] = ['simHei']
mpl.rcParams['axes.unicode_minus'] = False

iris_feature_E = 'sepal length', 'sepal width', 'petal length', 'petal width'
iris_feature = 'èŠ±è¼é•¿åº¦', 'èŠ±è¼å®½åº¦', 'èŠ±ç“£é•¿åº¦', 'èŠ±ç“£å®½åº¦'
iris_class = 'Iris-setosa', 'Iris-versicolor', 'Iris-virginica'

path = 'iris_classification/iris.data'  # æ•°æ®æ–‡ä»¶è·¯å¾„
data = pd.read_csv(path, header=None)
x = data[list(range(4))]
# y = pd.Categorical(data[4]).codes
y = LabelEncoder().fit_transform(data[4])
# ä¸ºäº†å¯è§†åŒ–ï¼Œä»…ä½¿ç”¨å‰ä¸¤åˆ—ç‰¹å¾
x = x[[0, 1]]
# x = x.iloc[:, :2]
x_train, x_test, y_train, y_test = train_test_split(x,
                                                    y,
                                                    test_size=0.3,
                                                    random_state=1)

# å†³ç­–æ ‘å‚æ•°ä¼°è®¡
# min_samples_split = 10ï¼šå¦‚æœè¯¥ç»“ç‚¹åŒ…å«çš„æ ·æœ¬æ•°ç›®å¤§äº10ï¼Œåˆ™(æœ‰å¯èƒ½)å¯¹å…¶åˆ†æ”¯
# min_samples_leaf = 10ï¼šè‹¥å°†æŸç»“ç‚¹åˆ†æ”¯åï¼Œå¾—åˆ°çš„æ¯ä¸ªå­ç»“ç‚¹æ ·æœ¬æ•°ç›®éƒ½å¤§äº10ï¼Œåˆ™å®Œæˆåˆ†æ”¯ï¼›å¦åˆ™ï¼Œä¸è¿›è¡Œåˆ†æ”¯
model = DecisionTreeClassifier(criterion='entropy')
model.fit(x_train, y_train)
y_train_pred = model.predict(x_train)
print('è®­ç»ƒé›†æ­£ç¡®ç‡ï¼š', accuracy_score(y_train, y_train_pred))
y_test_hat = model.predict(x_test)  # æµ‹è¯•æ•°æ®
print('æµ‹è¯•é›†æ­£ç¡®ç‡ï¼š', accuracy_score(y_test, y_test_hat))

# ä¿å­˜
# dot -Tpng my.dot -o my.png
# 1ã€è¾“å‡º
# with open('iris.dot', 'w') as f:
#     tree.export_graphviz(model, out_file=f, feature_names=iris_feature_E[0:2], class_names=iris_class,
#                          filled=True, rounded=True, special_characters=True)
tree.export_graphviz(model,
                     out_file='iris_classification/iris.dot',
                     feature_names=iris_feature_E[0:2],
                     class_names=iris_class,
                     filled=True,
                     rounded=True,
                     special_characters=True)
# 2ã€ç»™å®šæ–‡ä»¶å
# tree.export_graphviz(model, out_file='iris.dot')
# tree.export_graphviz(model, out_file='iris.dot')
# 3ã€è¾“å‡ºä¸ºpdfæ ¼å¼
dot_data = tree.export_graphviz(model,
                                out_file=None,
                                feature_names=iris_feature_E[0:2],
                                class_names=iris_class,
                                filled=True,
                                rounded=True,
                                special_characters=True)
graph = pydotplus.graph_from_dot_data(dot_data)
graph.write_pdf('iris_classification/iris.pdf')
f = open('iris_classification/iris.png', 'wb')
f.write(graph.create_png())
f.close()

# ç”»å›¾
N, M = 50, 50  # æ¨ªçºµå„é‡‡æ ·å¤šå°‘ä¸ªå€¼
x1_min, x2_min = x.min()
x1_max, x2_max = x.max()
t1 = np.linspace(x1_min, x1_max, N)
t2 = np.linspace(x2_min, x2_max, M)
x1, x2 = np.meshgrid(t1, t2)  # ç”Ÿæˆç½‘æ ¼é‡‡æ ·ç‚¹
x_show = np.stack((x1.flat, x2.flat), axis=1)  # æµ‹è¯•ç‚¹
print(x_show.shape)
print('x_show = \n', x_show)

cm_light = mpl.colors.ListedColormap(['#A0FFA0', '#FFA0A0', '#A0A0FF'])
cm_dark = mpl.colors.ListedColormap(['g', 'r', 'b'])
y_show_hat = model.predict(x_show)  # é¢„æµ‹å€¼
print(y_show_hat.shape)
print(y_show_hat)
y_show_hat = y_show_hat.reshape(x1.shape)  # ä½¿ä¹‹ä¸è¾“å…¥çš„å½¢çŠ¶ç›¸åŒ
print(y_show_hat)
plt.figure(facecolor='w')
plt.pcolormesh(x1, x2, y_show_hat, cmap=cm_light)  # é¢„æµ‹å€¼çš„æ˜¾ç¤º
plt.scatter(x_test[0],
            x_test[1],
            c=y_test.ravel(),
            edgecolors='k',
            s=100,
            zorder=10,
            cmap=cm_dark,
            marker='*')  # æµ‹è¯•æ•°æ®
plt.scatter(x[0], x[1], c=y.ravel(), edgecolors='k', s=20,
            cmap=cm_dark)  # å…¨éƒ¨æ•°æ®
plt.xlabel(iris_feature[0], fontsize=13)
plt.ylabel(iris_feature[1], fontsize=13)
plt.xlim(x1_min, x1_max)
plt.ylim(x2_min, x2_max)
plt.grid(b=True, ls=':', color='#606060')
plt.title('é¸¢å°¾èŠ±æ•°æ®çš„å†³ç­–æ ‘åˆ†ç±»', fontsize=15)
plt.show()

# è®­ç»ƒé›†ä¸Šçš„é¢„æµ‹ç»“æœ
y_test = y_test.reshape(-1)
print(y_test_hat)
print(y_test)
result = (y_test_hat == y_test)  # Trueåˆ™é¢„æµ‹æ­£ç¡®ï¼ŒFalseåˆ™é¢„æµ‹é”™è¯¯
acc = np.mean(result)
print('å‡†ç¡®åº¦: %.2f%%' % (100 * acc))

# è¿‡æ‹Ÿåˆï¼šé”™è¯¯ç‡
depth = np.arange(1, 15)
err_train_list = []
err_test_list = []
clf = DecisionTreeClassifier(criterion='entropy')
for d in depth:
    clf.set_params(max_depth=d)
    clf.fit(x_train, y_train)
    y_train_pred = clf.predict(x_train)
    err_train = 1 - accuracy_score(y_train, y_train_pred)
    err_train_list.append(err_train)
    y_test_pred = clf.predict(x_test)
    err_test = 1 - accuracy_score(y_test, y_test_pred)
    err_test_list.append(err_test)
    print(d, ' æµ‹è¯•é›†é”™è¯¯ç‡: %.2f%%' % (100 * err_test))
plt.figure(facecolor='w')
plt.plot(depth,
         err_test_list,
         'ro-',
         markeredgecolor='k',
         lw=2,
         label='æµ‹è¯•é›†é”™è¯¯ç‡')
plt.plot(depth,
         err_train_list,
         'go-',
         markeredgecolor='k',
         lw=2,
         label='è®­ç»ƒé›†é”™è¯¯ç‡')
plt.xlabel('å†³ç­–æ ‘æ·±åº¦', fontsize=13)
plt.ylabel('é”™è¯¯ç‡', fontsize=13)
plt.legend(loc='lower left', fontsize=13)
plt.title('å†³ç­–æ ‘æ·±åº¦ä¸è¿‡æ‹Ÿåˆ', fontsize=15)
plt.grid(b=True, ls=':', color='#606060')
plt.show()
```

    è®­ç»ƒé›†æ­£ç¡®ç‡ï¼š 0.9523809523809523
    æµ‹è¯•é›†æ­£ç¡®ç‡ï¼š 0.6
    (2500, 2)
    x_show = 
     [[4.3        2.        ]
     [4.37346939 2.        ]
     [4.44693878 2.        ]
     ...
     [7.75306122 4.4       ]
     [7.82653061 4.4       ]
     [7.9        4.4       ]]
    (2500,)
    [0 0 0 ... 2 2 2]
    [[0 0 0 ... 1 1 1]
     [0 0 0 ... 1 1 1]
     [0 0 0 ... 1 1 1]
     ...
     [0 0 0 ... 2 2 2]
     [0 0 0 ... 2 2 2]
     [0 0 0 ... 2 2 2]]
    

    C:\Users\utsuk\AppData\Local\Temp\ipykernel_24332\3076338488.py:91: MatplotlibDeprecationWarning: shading='flat' when X and Y have the same dimensions as C is deprecated since 3.3.  Either specify the corners of the quadrilaterals with X and Y, or pass shading='auto', 'nearest' or 'gouraud', or set rcParams['pcolor.shading'].  This will become an error two minor releases later.
      plt.pcolormesh(x1, x2, y_show_hat, cmap=cm_light)  # é¢„æµ‹å€¼çš„æ˜¾ç¤º
    


    
![png](ML_files/ML_16_2.png)
    


    [0 2 2 0 2 2 1 0 0 2 2 0 1 2 1 0 2 1 0 0 1 0 2 0 2 1 0 0 1 1 2 2 2 2 1 0 1
     0 2 1 2 0 1 1 1]
    [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1
     0 1 2 2 0 2 2 1]
    å‡†ç¡®åº¦: 60.00%
    1  æµ‹è¯•é›†é”™è¯¯ç‡: 44.44%
    2  æµ‹è¯•é›†é”™è¯¯ç‡: 40.00%
    3  æµ‹è¯•é›†é”™è¯¯ç‡: 20.00%
    4  æµ‹è¯•é›†é”™è¯¯ç‡: 24.44%
    5  æµ‹è¯•é›†é”™è¯¯ç‡: 24.44%
    6  æµ‹è¯•é›†é”™è¯¯ç‡: 26.67%
    7  æµ‹è¯•é›†é”™è¯¯ç‡: 35.56%
    8  æµ‹è¯•é›†é”™è¯¯ç‡: 40.00%
    9  æµ‹è¯•é›†é”™è¯¯ç‡: 37.78%
    10  æµ‹è¯•é›†é”™è¯¯ç‡: 40.00%
    11  æµ‹è¯•é›†é”™è¯¯ç‡: 35.56%
    12  æµ‹è¯•é›†é”™è¯¯ç‡: 35.56%
    13  æµ‹è¯•é›†é”™è¯¯ç‡: 37.78%
    14  æµ‹è¯•é›†é”™è¯¯ç‡: 40.00%
    


    
![png](ML_files/ML_16_4.png)
    


## ä»£ç ç›¸å…³

### å­˜å‚¨-è¯»å–æ¨¡å‹



```python
import numpy as np
import sklearn.linear_model as lm  # çº¿æ€§æ¨¡å‹
import sklearn.metrics as sm  # æ¨¡å‹æ€§èƒ½è¯„ä»·æ¨¡å—
import matplotlib.pyplot as mp
import pickle

x = np.array([[0.5], [0.6], [0.8], [1.1], [1.4]])  # è¾“å…¥é›†
y = np.array([5.0, 5.5, 6.0, 6.8, 7.0])  # è¾“å‡ºé›†

# åˆ›å»ºçº¿æ€§å›å½’å™¨
model = lm.LinearRegression()
# ç”¨å·²çŸ¥è¾“å…¥ã€è¾“å‡ºæ•°æ®é›†è®­ç»ƒå›å½’å™¨
model.fit(x, y)

print("è®­ç»ƒå®Œæˆ.")

# ä¿å­˜è®­ç»ƒåçš„æ¨¡å‹
with open('linear_model.pkl', 'wb') as f:
    pickle.dump(model, f)
    print("ä¿å­˜æ¨¡å‹å®Œæˆ.")


######################### åŠ è½½æ¨¡å‹ #########################
# ä¸Šé¢é€šè¿‡è®­ç»ƒæ•°æ®x,y è®­ç»ƒå¥½äº† x -> y çš„çº¿æ€§å›å½’æ¨¡å‹
# ä¸‹é¢åŠ è½½æ¨¡å‹, å†ç»™å‡ºæµ‹è¯•æ•°æ® x, æŸ¥çœ‹æ¨¡å‹é¢„æµ‹ç»“æœç›´çº¿ä¸åŸå§‹æ•°æ® (x,y) çš„æ‹Ÿåˆåº¦
with open('linear_model.pkl', 'rb') as f:
    model = pickle.load(f)
    print("åŠ è½½æ¨¡å‹å®Œæˆ.")

# æ ¹æ®åŠ è½½çš„æ¨¡å‹é¢„æµ‹è¾“å‡º
pred_y = model.predict(x)

# å¯è§†åŒ–å›å½’æ›²çº¿
mp.figure('Linear Regression', facecolor='lightgray')
mp.title('Linear Regression', fontsize=20)
mp.xlabel('x', fontsize=14)
mp.ylabel('y', fontsize=14)
mp.tick_params(labelsize=10)
mp.grid(linestyle=':')
mp.scatter(x, y, c='blue', alpha=0.8, s=60, label='Sample Points')

mp.plot(x, pred_y, c='orangered', label='Regression')

mp.legend()
mp.show()
```

    è®­ç»ƒå®Œæˆ.
    ä¿å­˜æ¨¡å‹å®Œæˆ.
    åŠ è½½æ¨¡å‹å®Œæˆ.
    


    
![png](ML_files/ML_18_1.png)
    


## ä¿¡æ¯è®º

### ä¸¤ç‚¹åˆ†å¸ƒä¿¡æ¯ç†µ



```python
import numpy as np
import matplotlib.pyplot as plt

# å› ä¸º ln0 æ— å®šä¹‰, ç”¨æ­¤å€¼æ¨¡æ‹Ÿè¶‹è¿‘ 0
eps = 1e-5

# probability
p = np.linspace(eps, 1 - eps, 100)

# Information entropy
h = -(1 - p) * np.log2(1 - p) - p * np.log2(p)

plt.plot(p, h, label='Information entropy', color='red', lw=3)
plt.xlabel('Probability', fontsize=16)
plt.ylabel('Entropy', fontsize=16)
plt.legend(loc='best', fontsize=16)
plt.grid(True)
plt.show()

# ç»“æœä¸­ä¿¡æ¯ç†µçš„å³°å€¼å–å†³äº log åº•æ•°, eä¸ºåº•æ—¶å³°å€¼ä¸º0.7å·¦å³, 2ä¸ºåº•å³°å€¼ä¸º1
```


    
![png](ML_files/ML_20_0.png)
    


<a>![åˆ†å‰²çº¿](https://cdn.jsdelivr.net/gh/Weidows/Images/img/divider.png)</a>

## å€Ÿç‰©è¡¨

<a name='cite_note-1' href='#cite_ref-1'>[1]</a>: https://discover304.top/

